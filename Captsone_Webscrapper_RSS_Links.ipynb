{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports the needed Libaries and the File (podcast_id_titles.csv)**"
      ],
      "metadata": {
        "id": "RC5RFLkSpTsB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU6eoqbkGP-A"
      },
      "outputs": [],
      "source": [
        "# Imports the functions needed for the scrapping\n",
        "from googlesearch import search\n",
        "import urllib\n",
        "import webbrowser \n",
        "from bs4 import BeautifulSoup as BS\n",
        "import io\n",
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the podcast_id_titles from your local directory\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "bccrIIdblMgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Converts the file to a df then creates a list with the titles\n",
        "df = pd.read_csv('podcast_id_titles.csv', delimiter=',')\n",
        "query_list = list(df['title'])\n",
        "query_list"
      ],
      "metadata": {
        "id": "387HpY2JCqX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DUCK DUCK GO SEARCH METHOD**\n",
        "ULTAMATELY NOT USED IN PROJECT BUT SAVED FOR FUTURE USE\n",
        "\n",
        "*   Usually only gets 10-20 Links at a time.\n",
        "*   Not as realiable as Google Search Method\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qq6g7bzbpHz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Duck Duck Go Search and returns the URL for the podcast addict page with the name of the podcast\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:84.0) Gecko/20100101 Firefox/84.0\",\n",
        "}\n",
        "\n",
        "duck_addict_list = []\n",
        "\n",
        "for name in query_list:\n",
        "    print(name)\n",
        "    # Does String Minipulation to get rid of spaces and weird characters\n",
        "    if name[0] == ' ': \n",
        "      name = name[1:]\n",
        "    elif name[-1] == ' ': \n",
        "      name = name[:-1]\n",
        "    name = name.replace(\" \",\"+\")\n",
        "    name = name.replace(\"'\",\"\")\n",
        "    name = name.replace(\",\",\"\")\n",
        "    name = name.replace(\"–\",\"\")\n",
        "    name = name.replace(\":\",\"\")\n",
        "    name = name.replace(\"™\",\"\")\n",
        "    name = name.replace(\"!\",\"\")\n",
        "    name = name.replace(\"(\",\"\")\n",
        "    name = name.replace(\")\",\"\")\n",
        "    name = name.replace(\"&\",\"and\")\n",
        "    name = name.replace(\"+++\",\"+\")\n",
        "    name = name.replace(\"++\",\"+\")\n",
        "\n",
        "    # Creates the URL that can be used in searching\n",
        "    url = \"https://duckduckgo.com/html/?q=podcast+addict+\"+name\n",
        "    page = requests.get(url, headers=headers).text\n",
        "    soup = BS(page, 'html.parser').find_all(\"a\", class_=\"result__url\")\n",
        "    i = 0\n",
        "    for link in soup:\n",
        "      if i < 1: \n",
        "        duck_addict_list.append(link['href'])\n",
        "        break\n",
        "# Prints the list of URLS found\n",
        "print(duck_addict_list)"
      ],
      "metadata": {
        "id": "7flbpJ2qwczA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the list of URLS found\n",
        "print(duck_addict_list)\n",
        "# Prints the Length of the URLS fouund\n",
        "print(len(duck_addict_list))\n"
      ],
      "metadata": {
        "id": "OgavhIZCOjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is meant to do some error checking to get rid of links that went to specfic episodes instead of the the actual podcast page\n",
        "for url in duck_addict_list:\n",
        "    if (url[0:34] != 'https://podcastaddict.com/podcast/'):\n",
        "      duck_addict_list.remove(url)\n",
        "\n",
        "print(duck_addict_list)\n",
        "print(len(duck_addict_list))"
      ],
      "metadata": {
        "id": "s4FN68r_KrbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GOOGLE SEARCH METHOD**\n",
        "Returns about 40 Links. Can run every 10-15 Min. Also can take a couple minutes to run based on WiFi Speeds. If you get a 429 Error that just means that it exceed the number of searches per a batch and stopped"
      ],
      "metadata": {
        "id": "AQM3yaYTpAuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Does a Google Search and returns the URL for the podcast addict page with the name of the podcast\n",
        "google_addict_list = []\n",
        "for name in query_list:\n",
        "    for result in search(name,  tld='com', lang='en', domains = [\"podcastaddict.com/podcast\"], num=2, start=0, stop=1, pause=3.0):\n",
        "        google_addict_list.append(result)\n",
        "\n",
        "print(google_addict_list)"
      ],
      "metadata": {
        "id": "vnE1bU6yG-jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_addict_list)\n",
        "print(len(google_addict_list))"
      ],
      "metadata": {
        "id": "n3Y4XL1-Wucy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PODCAST ADDICT SCRAPING**\n",
        "Using the Links that we got from Duck Duck Go or the Google Search method we will now scrap the link that directly goes to the RSS for each individual podcast from podcastaddicts.com\n",
        "\n",
        "THIS SITE IS UNREALIABLE AND CONSTANTLY CHANGING... IF YOU ENCOUNTER ANY ISSUES PLEASE REACH OUT. Proof can be provided that this script worked. "
      ],
      "metadata": {
        "id": "RIKza1XkqqFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Needed to open URL from podcastaddict.com\n",
        "header= {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
        "    'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
        "    'Chrome/23.0.1271.64 Safari/537.11',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "    'Accept-Encoding': 'none',\n",
        "    'Accept-Language': 'en-US,en;q=0.8',\n",
        "    'Connection': 'keep-alive'}"
      ],
      "metadata": {
        "id": "YvqQk-_RnojN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that opens the podcast addict link and returns the link associated with the RSS feed\n",
        "def get_rss(url): \n",
        "    # Makes the request and opens the addict link\n",
        "    req = urllib.request.Request(url, headers=header) \n",
        "    page = urllib.request.urlopen(req).read()\n",
        "    # Parses out the URL and returns the link to the RSS\n",
        "    soup = BS(page, \"html.parser\")\n",
        "    i = 0\n",
        "    for link in soup.find_all('a'):\n",
        "        if i == 30: \n",
        "          rss = link.get('href')\n",
        "          print(rss)\n",
        "          return rss\n",
        "        else: \n",
        "          i += 1\n",
        "  "
      ],
      "metadata": {
        "id": "hheqWc6n5wr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you get a 404 Error that just means that the script is being blocked from Podcast Addict for to many requests. Try again in a bit"
      ],
      "metadata": {
        "id": "hkPsTJyGPj8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that iterates through the Podcast Addict links and returns the RSS links for each podcast\n",
        "#duck_RSS_links = []\n",
        "google_RSS_links = []\n",
        "\n",
        "#for url in duck_addict_list: \n",
        "  #result = get_rss(url)\n",
        "  #duck_RSS_links.append(result)\n",
        "\n",
        "for url in google_addict_list: \n",
        "  result = get_rss(url)\n",
        "  google_RSS_links.append(result)\n",
        "\n",
        "#print(duck_RSS_links)\n",
        "print(google_RSS_links)\n",
        "print(len(google_RSS_links))"
      ],
      "metadata": {
        "id": "nVwJ6ruhX3Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Downloads the CSV to the desktop\n",
        "df = pd.DataFrame(google_RSS_links)\n",
        "df.to_csv('RSS_Links.csv', encoding = 'utf-8-sig') \n",
        "files.download('RSS_Links.csv')"
      ],
      "metadata": {
        "id": "_uSVz39ioJDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}